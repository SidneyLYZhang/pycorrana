#!/usr/bin/env python
"""
PyCorrAna æ¼”ç¤ºè„šæœ¬
==================
å±•ç¤º PyCorrAna çš„æ ¸å¿ƒåŠŸèƒ½
"""

import sys
import os

# æ·»åŠ å½“å‰ç›®å½•åˆ°è·¯å¾„
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))

import numpy as np
import pandas as pd

# å¯¼å…¥ PyCorrAna æ¨¡å—
from pycorrana import quick_corr, CorrAnalyzer
from pycorrana.core.partial_corr import partial_corr, partial_corr_matrix
from pycorrana.core.nonlinear import distance_correlation, mutual_info_score
from pycorrana.datasets import make_correlated_data, load_iris
from pycorrana.utils.data_utils import infer_types, handle_missing


def demo_basic_analysis():
    """æ¼”ç¤ºåŸºç¡€åˆ†æåŠŸèƒ½"""
    print("\n" + "=" * 70)
    print(" " * 20 + "PyCorrAna åŠŸèƒ½æ¼”ç¤º")
    print("=" * 70)
    
    print("\nğŸ“Š æ¼”ç¤º1: åŸºç¡€ç›¸å…³æ€§åˆ†æ")
    print("-" * 70)
    
    # ç”Ÿæˆæµ‹è¯•æ•°æ®
    df = make_correlated_data(n_samples=200, n_features=5, correlation=0.6)
    print(f"\nç”Ÿæˆæµ‹è¯•æ•°æ®: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
    print(f"åˆ—å: {', '.join(df.columns)}")
    
    # ä¸€é”®åˆ†æ
    print("\næ‰§è¡Œ quick_corr() åˆ†æ...")
    result = quick_corr(df, plot=False, verbose=True)
    
    # æ˜¾ç¤ºç»“æœ
    print("\nğŸ“ˆ ç›¸å…³ç³»æ•°çŸ©é˜µ:")
    print(result['correlation_matrix'].round(3).to_string())
    
    print("\nğŸ” Top 5 æ˜¾è‘—ç›¸å…³å¯¹:")
    for i, pair in enumerate(result['significant_pairs'][:5], 1):
        print(f"  {i}. {pair['var1']} vs {pair['var2']}")
        print(f"     ç›¸å…³ç³»æ•°: {pair['correlation']:.4f}")
        print(f"     på€¼: {pair['p_value']:.2e}")
        print(f"     æ–¹æ³•: {pair['method']}")
        print(f"     è§£é‡Š: {pair['interpretation']}")


def demo_auto_method_selection():
    """æ¼”ç¤ºè‡ªåŠ¨æ–¹æ³•é€‰æ‹©"""
    print("\n" + "=" * 70)
    print("ğŸ“Š æ¼”ç¤º2: è‡ªåŠ¨æ–¹æ³•é€‰æ‹©")
    print("-" * 70)
    
    # åˆ›å»ºä¸åŒç±»å‹çš„å˜é‡
    np.random.seed(42)
    n = 150
    
    df = pd.DataFrame({
        'numeric1': np.random.randn(n),
        'numeric2': np.random.randn(n),
        'binary': np.random.choice([0, 1], n),
        'category': np.random.choice(['A', 'B', 'C'], n),
    })
    
    # æ·»åŠ ç›¸å…³æ€§
    df['numeric2'] = df['numeric1'] * 0.7 + np.random.randn(n) * 0.5
    
    print("\næ•°æ®ç±»å‹:")
    type_mapping = infer_types(df)
    for col, t in type_mapping.items():
        print(f"  {col}: {t}")
    
    print("\næ‰§è¡Œè‡ªåŠ¨åˆ†æ...")
    analyzer = CorrAnalyzer(df, method='auto', verbose=True)
    analyzer.preprocess()
    analyzer.compute_correlation()
    
    print("\næ¯å¯¹å˜é‡ä½¿ç”¨çš„æ–¹æ³•:")
    for pair, method in analyzer.methods_used.items():
        print(f"  {pair}: {method}")


def demo_partial_correlation():
    """æ¼”ç¤ºåç›¸å…³åˆ†æ"""
    print("\n" + "=" * 70)
    print("ğŸ“Š æ¼”ç¤º3: åç›¸å…³åˆ†æ")
    print("-" * 70)
    
    # åˆ›å»ºæœ‰æ··æ·†å˜é‡çš„æ•°æ®
    np.random.seed(42)
    n = 200
    
    # Z æ˜¯æ··æ·†å˜é‡
    Z = np.random.randn(n)
    X = Z * 0.6 + np.random.randn(n)  # X ä¸ Z ç›¸å…³
    Y = Z * 0.6 + np.random.randn(n)  # Y ä¸ Z ç›¸å…³
    
    df = pd.DataFrame({'X': X, 'Y': Y, 'Z': Z})
    
    print("\næ•°æ®ç”Ÿæˆ: X å’Œ Y éƒ½å— Z å½±å“")
    
    # ç®€å•ç›¸å…³
    simple_corr = df[['X', 'Y']].corr().iloc[0, 1]
    print(f"\nç®€å•ç›¸å…³ç³»æ•° (X vs Y): {simple_corr:.4f}")
    
    # åç›¸å…³
    result = partial_corr(df, x='X', y='Y', covars='Z')
    print(f"åç›¸å…³ç³»æ•° (X vs Y, æ§åˆ¶ Z): {result['partial_correlation']:.4f}")
    print(f"på€¼: {result['p_value']:.4e}")
    print(f"95% ç½®ä¿¡åŒºé—´: [{result['ci_95'][0]:.4f}, {result['ci_95'][1]:.4f}]")
    
    print("\nè¯´æ˜: æ§åˆ¶ Z åï¼ŒX å’Œ Y çš„ç›¸å…³æ€§æ˜¾è‘—é™ä½ï¼Œè¯´æ˜ä¹‹å‰çš„ç›¸å…³ä¸»è¦ç”± Z å¼•èµ·")


def demo_nonlinear_detection():
    """æ¼”ç¤ºéçº¿æ€§ä¾èµ–æ£€æµ‹"""
    print("\n" + "=" * 70)
    print("ğŸ“Š æ¼”ç¤º4: éçº¿æ€§ä¾èµ–æ£€æµ‹")
    print("-" * 70)
    
    np.random.seed(42)
    n = 200
    x = np.random.randn(n)
    
    # çº¿æ€§å…³ç³»
    y_linear = x * 2 + np.random.randn(n) * 0.3
    
    # äºŒæ¬¡å…³ç³»ï¼ˆéçº¿æ€§ï¼‰
    y_quad = x**2 + np.random.randn(n) * 0.3
    
    df = pd.DataFrame({
        'x': x,
        'y_linear': y_linear,
        'y_quadratic': y_quad
    })
    
    print("\næ¯”è¾ƒçº¿æ€§å…³ç³»å’ŒäºŒæ¬¡å…³ç³»:")
    
    # Pearson ç›¸å…³
    pearson_linear = df[['x', 'y_linear']].corr().iloc[0, 1]
    pearson_quad = df[['x', 'y_quadratic']].corr().iloc[0, 1]
    
    print(f"\nPearson ç›¸å…³ç³»æ•°:")
    print(f"  çº¿æ€§å…³ç³»: {pearson_linear:.4f}")
    print(f"  äºŒæ¬¡å…³ç³»: {pearson_quad:.4f}")
    
    # è·ç¦»ç›¸å…³
    dcor_linear = distance_correlation(df['x'], df['y_linear'])
    dcor_quad = distance_correlation(df['x'], df['y_quadratic'])
    
    print(f"\nè·ç¦»ç›¸å…³ç³»æ•° (dCor):")
    print(f"  çº¿æ€§å…³ç³»: {dcor_linear['dcor']:.4f}")
    print(f"  äºŒæ¬¡å…³ç³»: {dcor_quad['dcor']:.4f}")
    
    # äº’ä¿¡æ¯
    mi_linear = mutual_info_score(df['x'], df['y_linear'])
    mi_quad = mutual_info_score(df['x'], df['y_quadratic'])
    
    print(f"\nå½’ä¸€åŒ–äº’ä¿¡æ¯ (MI):")
    print(f"  çº¿æ€§å…³ç³»: {mi_linear['mi_normalized']:.4f}")
    print(f"  äºŒæ¬¡å…³ç³»: {mi_quad['mi_normalized']:.4f}")
    
    print("\nè¯´æ˜: dCor å’Œ MI èƒ½æ›´å¥½åœ°æ£€æµ‹éçº¿æ€§å…³ç³»")


def demo_missing_value_handling():
    """æ¼”ç¤ºç¼ºå¤±å€¼å¤„ç†"""
    print("\n" + "=" * 70)
    print("ğŸ“Š æ¼”ç¤º5: ç¼ºå¤±å€¼å¤„ç†")
    print("-" * 70)
    
    np.random.seed(42)
    df = pd.DataFrame({
        'A': np.random.randn(100),
        'B': np.random.randn(100),
        'C': np.random.randn(100)
    })
    
    # éšæœºæ’å…¥ç¼ºå¤±å€¼
    missing_idx_A = np.random.choice(100, 10, replace=False)
    missing_idx_B = np.random.choice(100, 15, replace=False)
    df.loc[missing_idx_A, 'A'] = np.nan
    df.loc[missing_idx_B, 'B'] = np.nan
    
    print(f"\nåŸå§‹æ•°æ®ç¼ºå¤±å€¼:")
    print(f"  Aåˆ—: {df['A'].isnull().sum()} ä¸ª")
    print(f"  Båˆ—: {df['B'].isnull().sum()} ä¸ª")
    print(f"  Cåˆ—: {df['C'].isnull().sum()} ä¸ª")
    
    # ä½¿ç”¨ä¸­ä½æ•°å¡«å……
    df_filled = handle_missing(df, strategy='fill', fill_method='median', verbose=True)
    
    print(f"\nå¡«å……åç¼ºå¤±å€¼:")
    print(f"  Aåˆ—: {df_filled['A'].isnull().sum()} ä¸ª")
    print(f"  Båˆ—: {df_filled['B'].isnull().sum()} ä¸ª")
    print(f"  Cåˆ—: {df_filled['C'].isnull().sum()} ä¸ª")


def demo_real_dataset():
    """æ¼”ç¤ºçœŸå®æ•°æ®é›†åˆ†æ"""
    print("\n" + "=" * 70)
    print("ğŸ“Š æ¼”ç¤º6: çœŸå®æ•°æ®é›†åˆ†æ (Iris)")
    print("-" * 70)
    
    df = load_iris()
    print(f"\nIris æ•°æ®é›†: {df.shape[0]} è¡Œ Ã— {df.shape[1]} åˆ—")
    print(f"ç‰¹å¾: {', '.join(df.columns[:-1])}")
    print(f"ç±»åˆ«: {df['species'].unique()}")
    
    # åˆ†æ
    result = quick_corr(df, plot=False, verbose=True)
    
    print("\nç‰¹å¾é—´ç›¸å…³æ€§ (Top 5):")
    for i, pair in enumerate(result['significant_pairs'][:5], 1):
        if 'species' not in [pair['var1'], pair['var2']]:
            print(f"  {i}. {pair['var1']} vs {pair['var2']}: {pair['correlation']:.4f}")


def main():
    """ä¸»å‡½æ•°"""
    print("\n" + "=" * 70)
    print("  PyCorrAna - Python Correlation Analysis Toolkit")
    print("  è‡ªåŠ¨åŒ–ç›¸å…³æ€§åˆ†æå·¥å…·")
    print("=" * 70)
    
    # è¿è¡Œæ‰€æœ‰æ¼”ç¤º
    demo_basic_analysis()
    demo_auto_method_selection()
    demo_partial_correlation()
    demo_nonlinear_detection()
    demo_missing_value_handling()
    demo_real_dataset()
    
    print("\n" + "=" * 70)
    print(" " * 20 + "æ¼”ç¤ºå®Œæˆï¼")
    print("=" * 70)
    print("\næ›´å¤šåŠŸèƒ½è¯·æŸ¥çœ‹:")
    print("  - ç¤ºä¾‹ä»£ç : examples/basic_usage.py")
    print("  - äº¤äº’å¼å·¥å…·: pycorrana-interactive")
    print("  - å‘½ä»¤è¡Œå·¥å…·: pycorrana --help")
    print("=" * 70 + "\n")


if __name__ == '__main__':
    main()
